{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a16ca4f9-37be-49a8-a8b4-0058a340d46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os, email, string, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20ea6ba9-d4ba-4ef8-9cb4-fcf43a97810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract email body from raw email\n",
    "def get_email_body(email_string):\n",
    "    # parse emial\n",
    "    email_body = email.message_from_string(email_string)\n",
    "    # only keep the main email body, discard any attachments\n",
    "    for i in email_body.walk():\n",
    "        if i.get_content_type() == 'text/plain':\n",
    "            # get payload in bytes\n",
    "            body = i.get_payload(decode=True)\n",
    "            # decode utf-8 character set\n",
    "            try:\n",
    "                return body.decode('windows-1252')\n",
    "            except UnicodeDecodeError:\n",
    "                # decode windows-1252 character set\n",
    "                return ''\n",
    "    # return empty string if main body not found\n",
    "    return ''\n",
    "# remove the stop words from email body\n",
    "def remove_stop_words(text, stop_words):\n",
    "    # split sentences into tokens for individual identical check of stop_words \n",
    "    text_tokens = text.split()\n",
    "    filtered_tokens = [token for token in text_tokens if token not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# extract features from email body\n",
    "def extract_features(text):\n",
    "    # convert to lowercase and remove punctuation\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # split text into words\n",
    "    words = text.split()\n",
    "    # count word frequencies\n",
    "    return Counter(words)\n",
    "# load stop words\n",
    "with open('stop_words.txt', 'r') as f:\n",
    "    stop_words = f.read().split()\n",
    "    \n",
    "# initialize data structures\n",
    "emails = []\n",
    "labels = []\n",
    "features = []\n",
    "# read in labels\n",
    "skip_count = 0\n",
    "with open('labels', 'r') as label_n_file_path:\n",
    "    for line in label_n_file_path:\n",
    "        try:\n",
    "            # extract label and file path\n",
    "            label, file_path = line.strip().split(' ../')\n",
    "#             colon, normalized_path = file_path.split('../')\n",
    "            # read in email\n",
    "            with open(file_path, 'r') as email_file:\n",
    "                raw_email = email_file.read()\n",
    "                \n",
    "                body = get_email_body(raw_email)\n",
    "                \n",
    "                body = remove_stop_words(body, stop_words)\n",
    "\n",
    "                emails.append(raw_email)\n",
    "\n",
    "                labels.append(label)\n",
    "\n",
    "                features.append(extract_features(body))\n",
    "                \n",
    "        except UnicodeDecodeError:\n",
    "            # due to other unrecognizable formats, skip that file\n",
    "            skip_count += 1\n",
    "            \n",
    "# split data into training and testing sets \n",
    "# data reduced down to 36747 emails\n",
    "X_train=features[:20659]\n",
    "y_train=labels[:20659]\n",
    "X_test=features[20659:]\n",
    "y_test = labels[20659:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "297b5568-dc45-4022-bb65-0de5fd48a17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#folders 0-70 emails 20659\n",
    "#folders 0-70 emails 16088"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93c86e8",
   "metadata": {},
   "source": [
    "# Creating Feature Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "201deff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of top 10000 most common words\n",
    "word_counts = Counter()\n",
    "# get most common words/ 10k top words\n",
    "for email_features in features:\n",
    "    for word, count in email_features.items():\n",
    "        word_counts[word] += count\n",
    "top_words = [word for word, count in word_counts.most_common(10000)]\n",
    "\n",
    "# create feature matrix for training data\n",
    "#initialize matrix w/ all zeros initialy## %^remove\n",
    "X_train_matrix = np.zeros((len(X_train), len(top_words)))\n",
    "# turn to 2D array matrix with x_train rows, top_w colunm\n",
    "X_train_matrix = X_train_matrix.reshape(len(X_train), len(top_words))\n",
    "#check if word in top_words appeared in email\n",
    "for i, email_features in enumerate(X_train):\n",
    "    for j, word in enumerate(top_words):\n",
    "        if word in email_features:\n",
    "            X_train_matrix[i, j] = 1\n",
    "\n",
    "# create feature matrix for testing data\n",
    "X_test_matrix = np.zeros((len(X_test), len(top_words)))\n",
    "#\n",
    "X_test_matrix = X_test_matrix.reshape(len(X_test), len(top_words))\n",
    "#\n",
    "for i, email_features in enumerate(X_test):\n",
    "    for j, word in enumerate(top_words):\n",
    "        if word in email_features:\n",
    "            X_test_matrix[i, j] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844aebec",
   "metadata": {},
   "source": [
    "# Computing Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eaad5552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior probability of spam:  0.6360424028268551 prior probability of ham:  0.36395759717314485\n"
     ]
    }
   ],
   "source": [
    "# count number of spam and ham emails in training set\n",
    "num_spam = sum(1 for label in y_train if label == 'spam')\n",
    "num_ham = sum(1 for label in y_train if label == 'ham')\n",
    "\n",
    "# calculate prior probabilities\n",
    "# X_train or y_train can be use, it will have same len of emails\n",
    "prior_prob_spam = num_spam / len(X_train)\n",
    "prior_prob_ham = num_ham / len(X_train)\n",
    "print('prior probability of spam: ', prior_prob_spam,'prior probability of ham: ', prior_prob_ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e20da16",
   "metadata": {},
   "source": [
    "# Computing the Likelihood of each word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec6eae89",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Computing the Likelihood of each word\n",
    "# create dictionary of word counts for spam and ham emails\n",
    "spam_word_counts = Counter()\n",
    "ham_word_counts = Counter()\n",
    "\n",
    "for i, label in enumerate(y_train):\n",
    "    if label == 'spam':\n",
    "        for word, count in X_train[i].items():\n",
    "            spam_word_counts[word] += count\n",
    "    else:\n",
    "        for word, count in X_train[i].items():\n",
    "            ham_word_counts[word] += count\n",
    "\n",
    "# apply Laplace smoothing\n",
    "lambda_ = 1\n",
    "vocab_size = len(top_words)\n",
    "\n",
    "# compute likelihood of each word given spam\n",
    "likelihood_spam = {}\n",
    "for word in top_words:\n",
    "    likelihood_spam[word] = (spam_word_counts[word] + lambda_) / (num_spam + lambda_ * vocab_size)\n",
    "\n",
    "# compute likelihood of each word given ham\n",
    "likelihood_ham = {}\n",
    "for word in top_words:\n",
    "    likelihood_ham[word] = (ham_word_counts[word] + lambda_) / (num_ham + lambda_ * vocab_size)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cc4b56",
   "metadata": {},
   "source": [
    "# Testing the Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e743392",
   "metadata": {},
   "source": [
    "## Test classifier with unknown message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "845c765d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the NaiveBayesClassifier\n",
    "classifier = MultinomialNB()\n",
    "cv = CountVectorizer()\n",
    "# Train the classifier\n",
    "classifier.fit(X_train_matrix, y_train)\n",
    "\n",
    "# fed with unknown message I got from the test dataset\n",
    "with open('data/071/001', 'r') as f:\n",
    "    raw_email = f.read()\n",
    "# Extract the features for the unknown message\n",
    "body = get_email_body(raw_email)\n",
    "# Remove stop words\n",
    "body = remove_stop_words(body, stop_words)\n",
    "# Extract features from the message\n",
    "features = extract_features(body)\n",
    "\n",
    "unknown_matrix = np.zeros((1, len(top_words)))\n",
    "for j, word in enumerate(top_words):\n",
    "    if word in word_counts:\n",
    "        unknown_matrix[0, j] = word_counts[word]\n",
    "prediction = classifier.predict(unknown_matrix.reshape(-1, len(top_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44698a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels for the test data\n",
    "predictions = classifier.predict(X_test_matrix.reshape(-1, len(top_words))) #0-topwords\n",
    "\n",
    "# Calculate the accuracy of the classifier using the test set\n",
    "# can use the score function of the sk-learn to return mean accuracy or can use .mean()\n",
    "accuracy = classifier.score(X_test_matrix, y_test)\n",
    "# print(accuracy) = 0.9320611636001989\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aebe1d",
   "metadata": {},
   "source": [
    "# Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98f6bcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9315638985579314 0.8610032089174126 0.9482886904761905\n",
      "fpr= 0.05171130952380952 fnr= 0.07682972367438387 tpr= 0.9231702763256161 tnr= 0.9482886904761905\n"
     ]
    }
   ],
   "source": [
    "# performance evaluation of test data\n",
    "accuracy = metrics.accuracy_score(y_test,predictions)\n",
    "precision = metrics.precision_score(y_test,predictions,pos_label='ham')\n",
    "recall = metrics.recall_score(y_test,predictions,pos_label='ham')\n",
    "\n",
    "cm = confusion_matrix(y_test,predictions)\n",
    "tn,fp,fn,tp = cm.ravel()\n",
    "\n",
    "fpr = fp/(fp+tn)\n",
    "fnr = fn/(tp+fn)\n",
    "tpr = tp/(tp+fn)\n",
    "tnr = tn/(tn+fp)\n",
    "print(accuracy,precision,recall)\n",
    "print(\"fpr=\",fpr,\"fnr=\",fnr,\"tpr=\",tpr,\"tnr=\",tnr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b64d940",
   "metadata": {},
   "source": [
    "# effect of removing stop words in terms of precision, recall, and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "685b0270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5468672302337145 0.39067854694996573 0.6361607142857143\n"
     ]
    }
   ],
   "source": [
    "def extract_features_without_stop_words(text):\n",
    "    # convert to lowercase and remove punctuation\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # split text into words\n",
    "    words = text.split()\n",
    "    # remove stop words\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # create a dictionary of word counts\n",
    "    word_counts = Counter(words)\n",
    "    # return the dictionary as a regular python dictionary (not a Counter object)\n",
    "    return dict(word_counts)\n",
    "features_with_stopwords=[]\n",
    "with open('labels', 'r') as label_n_file_path:\n",
    "    for line in label_n_file_path:\n",
    "        try:\n",
    "            # extract label and file path\n",
    "            label, file_path = line.strip().split(' ../')\n",
    "#             colon, normalized_path = file_path.split('../')\n",
    "            # read in email\n",
    "            with open(file_path, 'r') as email_file:\n",
    "                raw_email = email_file.read()\n",
    "                \n",
    "                body = get_email_body(raw_email)\n",
    "                \n",
    "                emails.append(raw_email)\n",
    "\n",
    "                labels.append(label)\n",
    "\n",
    "                features_with_stopwords.append(extract_features_without_stop_words(body))\n",
    "                \n",
    "        except UnicodeDecodeError:\n",
    "            # due to other unrecognizable formats, skip that file\n",
    "            skip_count += 1\n",
    "# create dictionary of top 10000 most common words\n",
    "word_counts_wsw = Counter()\n",
    "# get most common words/ 10k top words\n",
    "for email_features in features_with_stopwords:\n",
    "    for word, count in email_features.items():\n",
    "        word_counts_wsw[word] += count\n",
    "top_words_wsw = [word for word, count in word_counts_wsw.most_common(10000)]\n",
    "\n",
    "\n",
    "# create feature matrix for training data\n",
    "#initialize matrix w/ all zeros initialy## %^remove\n",
    "X_train_matrix_wsw = np.zeros((len(X_train), len(top_words_wsw)))\n",
    "# turn to 2D array matrix with x_train rows, top_w colunm\n",
    "X_train_matrix_wsw = X_train_matrix_wsw.reshape(len(X_train), len(top_words_wsw))\n",
    "#check if word in top_words appeared in email\n",
    "for i, email_features in enumerate(X_train):\n",
    "    for j, word in enumerate(top_words_wsw):\n",
    "        if word in email_features:\n",
    "            X_train_matrix_wsw[i, j] = 1\n",
    "\n",
    "# create feature matrix for testing data\n",
    "X_test_matrix_wsw = np.zeros((len(X_test), len(top_words_wsw)))\n",
    "#\n",
    "X_test_matrix_wsw = X_test_matrix_wsw.reshape(len(X_test), len(top_words_wsw))\n",
    "#\n",
    "for i, email_features in enumerate(X_test):\n",
    "    for j, word in enumerate(top_words_wsw):\n",
    "        if word in email_features:\n",
    "            X_test_matrix_wsw[i, j] = 1\n",
    "            \n",
    "# Predict the labels for the test data\n",
    "predictions_wsw = classifier.predict(X_test_matrix_wsw.reshape(-1, len(top_words_wsw))) #0-topwords\n",
    "\n",
    "# Calculate the accuracy of the classifier using the test set\n",
    "# can use the score function of the sk-learn to return mean accuracy or can use .mean()\n",
    "\n",
    "\n",
    "accuracy_wsw = metrics.accuracy_score(y_test,predictions_wsw)\n",
    "precision_wsw = metrics.precision_score(y_test,predictions_wsw,pos_label='ham')\n",
    "recall_wsw = metrics.recall_score(y_test,predictions_wsw,pos_label='ham')\n",
    "\n",
    "cm_wsw = confusion_matrix(y_test,predictions_wsw)\n",
    "tn,fp,fn,tp = cm_wsw.ravel()\n",
    "\n",
    "fpr_wsw = fp/(fp+tn)\n",
    "fnr_wsw = fn/(tp+fn)\n",
    "tpr_wsw = tp/(tp+fn)\n",
    "tnr_wsw = tn/(tn+fp)\n",
    "print(accuracy_wsw,precision_wsw,recall_wsw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecc7a7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3638392857142857 0.4979462285287528 0.5020537714712472 0.6361607142857143\n"
     ]
    }
   ],
   "source": [
    "print(fpr_wsw,fnr_wsw,tpr_wsw,tnr_wsw)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
